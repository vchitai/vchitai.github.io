_Nhiều phần trong bài này là kiến thức trong bài giảng của môn Big Data CTT339 của trường Đại học Khoa học Tự nhiên Thành phố Hồ Chí Minh_

# Low-level API của Spark

## Chúng là gì?

- bao gồm các Resilient Distributed Dataset (RDDs) có chứng năng điều khiển, sử dụng các dữ liệu phân tán
- Các biến broadcast và các accumulator chuyên phân phối và xử lý các biến chia sẻ phân tán

## Khi nào nên sử dụng?

- Khi chúng ta cần phải xử lý các chức năng không thể làm được trong các API high-level
  Vd: cần xử lý dữ liệu ở tầng vật lý - chủ động phân phối dữ liệu giữa các cluster
- Chỉnh sửa một số codebase tầng thấp viết bằng RDD
- Thực hiện một số công việc yêu cầu sử dụng các biến chia sẻ custom giữa các cluster

_Trong đa số trường hợp, sử dụng DataFrame sẽ hiệu quả hơn, ổn định hơn nhưng cũng tốn kém hơn so với RDD_

## Tại sao cần quan tâm đến chúng?

- Thật sự tất cả công việc sử dụng Spark rồi sẽ được biên dịch xuống thành các thành phần cơ bản. Các transformation trên DataFrame rồi cũng sẽ được biên dịch thành các transformation trên RDD

## Sử dụng chúng như thế nào?

- SparkContext sẽ là đầu vào của các chức năng liên quan đến các API low-level. SparkContext có thể được truy xuất thông qua SparkSession - công cụ mà chúng ta sử dụng để thực hiện tính toán giữa các cluster của Spark.

# Về RDD

## Các loại RDD

- Các RDD biểu diễn một tập hợp cố định, đã được phân vùng các record để có thể xử lý song song
- Các record trong RDD có thể là đối tượng Java, Scale hay Python tùy lập trình viên chọn. Không giống như DataFrame, mỗi record của DataFrame phải là một dòng có cấu trúc chứa các field đã được định nghĩa sẵn.
- RDD đã từng là API chính được sử dụng trong series Spark 1.x và vẫn có thể sử dụng trong version 2.X nhưng không còn được dùng thường xuyên nữa
- RDD API có thể được sử dụng trong Python, Scala hay Java
  - Scala và Java: Perfomance tương đương trên hầu hết mọi phần. (Chi phí lớn nhất là khi xử lý các raw object)
  - Python: Mất một lượng performance, chủ yếu là cho việc serialization giữa tiến trình Python và JVM

## Lưu ý khi sử dụng RDD

_Vì Spark không nhiều cấu trúc nội hàm của record như các API có cấu trúc khác nên_

- Rất mạnh nhưng cũng tiềm tàng nhiều nguy cơ
- Các object có thể chứa bất kỳ thông tin gì dưới bất kỳ format nào
- Các cách xử lý và tương tác với các giá trị phải được thực hiện thủ công
- Việc tối ưu hóa yêu cầu phải làm rất nhiều công việc thủ công

## Các loại RDD

- Có rất nhiều lớp con của RDD, hầu hết chúng là để cho DataFrame có thể tạo và tối ưu hóa các execution plan trên tầng vật lý (Xem cách hoạt động của Spark...)

- Người dùng thường chỉ sử dụng hai loại RDD sau:

  - Generic RDD
  - Key-value RDD

- Key-value RDD có nhiều hàm hỗ trợ đặc biệt cũng như hỗ trợ phân hoạch custom bằng key

## Thuộc tính của RDD

Bắt buộc:

- Một danh sách các partitions
- Một hàm để tính toán các split
- Một danh sách dependencies với RDD khác

Không bắt buộc:

- Một partitioner đối với RDD key-value (Trong trường hợp RDD được hash-partitioned)
- Một danh sách các vị trí lưu trữ để tính toán các split (Ví dụ như các block của HDFS)

## Tạo một RDD

- Cách đơn giản nhất: Tạo từ một DataFrame hay Dataset
  - Java hay Scala: Từ Dataset[T] có thể chuyển qua RDD[T]
  - Python: Chỉ có thể chuyển từ DataFrame[Row] sang RDD[Row]
    Đồng thời ta cũng có thể chuyển đổi theo chiều ngược lại
- Tạo từ một Local Collection: Sử dụng hàm parallelize trên SparkContext để chuyển một collection single node sang một collection parallel
- Tạo từ các Data Source: RDD không có khái niệm "Data Source APIs" vì chúng chủ yếu define các cấu trúc dependencies giữa các partition. Ta có thể tạo một RDD với mỗi dòng của text file là một record hay cả text file là một record duy nhất.

# Các transformation và action với RDD

RDD cung cấp các transformation và action hoạt động giống như DataFrame lẫn DataSets. Transformation xử lý các thao tác lazily và action xử lý thao tác ngay.

## Một số transformation

_Nhiều phiên bản transformation của RDD có thể hoạt động trên các Structured API, transformation xử lý lazily, tức là chỉ giúp dựng execution plans, dữ liệu chỉ được truy xuất thực sự khi thực hiện action_

- distinct: loại bỏ trùng lắp trong RDD
- filter: tương đương với việc sử dụng where trong SQL - tìm các record trong RDD xem những phần tử nào thỏa điều kiện. Có thể cung cấp một hàm phức tạp sử dụng để filter các record cần thiết - Như trong Python, ta có thể sử dụng hàm lambda để truyền vào filter
- map: thực hiện một công việc nào đó trên toàn bộ RDD. Trong Python sử dụng lambda với từng phần tử để truyền vào map
- flatMap: cung cấp một hàm đơn giản hơn hàm map. Yêu cầu output của map phải là một structure có thể lặp và mở rộng được.
- sortBy: mô tả một hàm để trích xuất dữ liệu từ các object của RDD và thực hiện sort được từ đó.
- randomSplit: nhận một mảng trọng số và tạo một random seed, tách các RDD thành một mảng các RDD có số lượng chia theo trọng số.

## Một số action

_Action thực thi ngay các transformation đã được thiết lập để thu thập dữ liệu về driver để xử lý hoặc ghi dữ liệu xuống các công cụ lưu trữ_

- reduce: thực hiện hàm reduce trên RDD để thu về 1 giá trị duy nhất
- count: đếm số dòng trong RDD
- countApprox: phiên bản đếm xấp xỉ của count, nhưng phải cung cấp timeout vì có thể không nhận được kết quả.
- countByValue: đếm số giá trị của RDD
  - chỉ sử dụng nếu map kết quả nhỏ vì tất cả dữ liệu sẽ được load lên memory của driver để tính toán
  - chỉ nên sử dụng trong tình huống số dòng nhỏ và số lượng item khác nhau cũng nhỏ.
- countApproxDistinct: đếm xấp xỉ các giá trị khác nhau
- countByValueApprox: đếm xấp xỉ các giá trị
- first: lấy giá trị đầu tiên của dataset
- max và min: lần lượt lấy giá trị lớn nhất và nhỏ nhất của dataset
- take và các method tương tự: lấy một lượng giá trị từ trong RDD. take sẽ trước hết scan qua một partition và sử dụng kết quả để dự đoán số lượng partition cần phải lấy thêm để thỏa mãn số lượng lấy.
- top và takeOrdered: top sẽ hiệu quả hơn takeOrdered vì top lấy các giá trị đầu tiên được sắp xếp ngầm trong RDD.
- takeSamples: lấy một lượng giá trị ngẫu nhiên trong RDD

## Một số kỹ thuật nâng cao đối với RDD:

### Lưu trữ file

- Lưu trữ file có nghĩa là ghi vào các file plain-text
- Có thể sử dụng các codec nén từ thư viện của Hadoop
- Lưu trữ vào các database bên ngoài yêu cầu ta phải lặp qua tất cả partition của RDD - Công việc được thực hiện ngầm trong các high-level API
- sequenceFile là một flat file chứa các cặp key-value, thường được sử dụng làm định dạng input/output của MapReduce. Spark có thể ghi các sequenceFile bằng các ghi lại các cặp key-value
- Đồng thời, Spark cũng hỗ trợ ghi nhiều định dạng file khác nhau, cho phép define các class, định dạng output, config và compression scheme của Hadoop.

### Caching - tăng tốc xử lý bằng cache

- Caching với RDD, Dataset hay DataFrame có nguyên lý như nhau.
- Chúng ta có thể lựa chọn cache hay persist một RDD, và mặc định, chỉ xử lý dữ liệu trong bộ nhớ

### Checkpointing - lưu trữ lại các bước xử lý để phục hồi

- Checkpointing lưu RDD vào đĩa cứng để các tiến trình khác để thể sử dụng lại RDD point này làm partition trung gian thay vì tính toán lại RDD từ các nguồn dữ liệu gốc
- Checkpointing cũng tương tự như cache, chỉ khác nhau là lưu trữ vào đĩa cứng và không dùng được trong API của DataFrame
- Chúng ta sẽ cần sử dụng nhiều để tối ưu tính toán.
